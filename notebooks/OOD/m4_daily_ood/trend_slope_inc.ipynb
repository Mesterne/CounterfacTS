{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a969d5-62ac-4b69-810d-cd723e9f3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590ce29c-8176-47a7-863e-b6a94dd65f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/whatif/lib/python3.8/site-packages/gluonts/json.py:45: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.loader import TrainDataLoader, ValidationDataLoader\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.time_feature import (\n",
    "    HourOfDay,\n",
    "    DayOfWeek,\n",
    "    DayOfMonth,\n",
    "    DayOfYear,\n",
    "    MonthOfYear\n",
    ")\n",
    "from gluonts.torch.batchify import batchify\n",
    "from gluonts.transform import (\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    ValidationSplitSampler\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.app.utils import get_prediction_dataloader\n",
    "from src.models.utils import get_model\n",
    "from src.utils.data_loading import load_features, load_score, load_test_data, load_train_data\n",
    "from src.utils.evaluation import score_batch\n",
    "from src.utils.features import trend_determination, trend_slope, trend_linearity, seasonal_determination, decomps_and_features, extract_trend\n",
    "from src.utils.transformations import manipulate_trend_component, manipulate_seasonal_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf78e58-abf1-4f0b-a5a1-4a9d3368f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nbeats_g\"\n",
    "dataset = \"m4_yearly\"\n",
    "\n",
    "datadir = f\"data/{dataset}\"\n",
    "experiment_dir = f\"experiments/{dataset}/{model_name}\"\n",
    "generated_datadir = os.path.join(f\"/datadrive2/whatif/{dataset}\", \"generated\", \"test\")\n",
    "metric = \"mase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d4ab5f-d442-4e7d-95eb-f67bb76b1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_dir, \"config.yaml\"), \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b67b45-6d97-41e3-a606-1628b5d8cb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = load_score(experiment_dir, metric)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e98aa35-fbef-4397-9d61-f9017414f74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25600, 4), (23000, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = load_features(datadir, train=True)\n",
    "test_features = load_features(datadir, train=False)\n",
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0bd136-9753-4149-9434-2912c19fbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "norm_train_features = scaler.fit_transform(train_features)\n",
    "norm_test_features = scaler.transform(test_features)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "train_pca_data = pca.fit_transform(norm_train_features)\n",
    "test_pca_data = pca.transform(norm_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53be8d4d-6d6c-4687-8ea0-8a48c09dcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_plot_from_dataloader(dataloader, sp, scaler, pca, train_pca_data, test_pca_data, num_samples=20000, lim=True):\n",
    "    sampled_series = []\n",
    "    for batch in dataloader:\n",
    "        data = torch.cat([batch[\"past_target\"], batch[\"future_target\"]], dim=-1).numpy()\n",
    "        for ts in data:\n",
    "            ts = pd.Series(data=ts)\n",
    "            sampled_series.append(ts)\n",
    "\n",
    "        if len(sampled_series) > num_samples:\n",
    "            break\n",
    "\n",
    "    _, sampled_features = decomps_and_features(sampled_series, sp)\n",
    "    sampled_scaled_features = scaler.transform(sampled_features)\n",
    "    sampled_pca_data = pca.transform(sampled_scaled_features)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    if train_pca_data is not None:\n",
    "        plt.scatter(train_pca_data[:, 0], train_pca_data[:, 1], label=\"train data\", s=5, alpha=.1)\n",
    "    if test_pca_data is not None:\n",
    "        plt.scatter(test_pca_data[:, 0], test_pca_data[:, 1], label=\"test data\", s=5, alpha=.1)\n",
    "    \n",
    "    plt.scatter(sampled_pca_data[:, 0], sampled_pca_data[:, 1], label=\"transformed train data\", s=5, alpha=.1, color=\"r\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if lim:\n",
    "        plt.xlim([-7.5, 7.5])\n",
    "        plt.ylim([-10, 10])\n",
    "    \n",
    "    return sampled_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8afb277a-c2a7-4539-89ed-964611552bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataloader(dataset, context_length, prediction_length, batch_size, num_batches_per_epoch):\n",
    "    transformation = Chain([\n",
    "        AddObservedValuesIndicator(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.OBSERVED_VALUES,\n",
    "        ),\n",
    "        AddTimeFeatures(\n",
    "            start_field=FieldName.START,\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            pred_length=prediction_length,\n",
    "            time_features=[HourOfDay(), DayOfWeek(), DayOfMonth(), DayOfYear(), MonthOfYear()]\n",
    "        ),\n",
    "        InstanceSplitter(\n",
    "            target_field=FieldName.TARGET,\n",
    "            is_pad_field=FieldName.IS_PAD,\n",
    "            start_field=FieldName.START,\n",
    "            forecast_start_field=FieldName.FORECAST_START,\n",
    "            instance_sampler=ExpectedNumInstanceSampler(num_instances=1, min_past=context_length,\n",
    "                                                        min_future=prediction_length),\n",
    "            past_length=context_length,\n",
    "            future_length=prediction_length,\n",
    "            time_series_fields=[FieldName.FEAT_TIME, FieldName.OBSERVED_VALUES]\n",
    "        )\n",
    "    ])\n",
    "    dataloader = TrainDataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        stack_fn=batchify,\n",
    "        transform=transformation,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "        num_workers=1\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc63b0d8-7304-47c5-b1b5-16333cbea15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_dataloader(dataset, context_length, prediction_length, batch_size, num_batches_per_epoch):\n",
    "    transformation = Chain([\n",
    "        AddObservedValuesIndicator(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.OBSERVED_VALUES,\n",
    "        ),\n",
    "        AddTimeFeatures(\n",
    "            start_field=FieldName.START,\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            pred_length=prediction_length,\n",
    "            time_features=[HourOfDay(), DayOfWeek(), DayOfMonth(), DayOfYear(), MonthOfYear()]\n",
    "        ),\n",
    "        InstanceSplitter(\n",
    "            target_field=FieldName.TARGET,\n",
    "            is_pad_field=FieldName.IS_PAD,\n",
    "            start_field=FieldName.START,\n",
    "            forecast_start_field=FieldName.FORECAST_START,\n",
    "            instance_sampler=ValidationSplitSampler(min_future=prediction_length),\n",
    "            past_length=context_length,\n",
    "            future_length=prediction_length,\n",
    "            time_series_fields=[FieldName.FEAT_TIME, FieldName.OBSERVED_VALUES]\n",
    "        )\n",
    "    ])\n",
    "    dataloader = ValidationDataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        stack_fn=batchify,\n",
    "        transform=transformation,\n",
    "        num_workers=1\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef90696a-7328-4468-8ca9-61930282a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_val_data(dataset, num_validation_windows, context_length, prediction_length):\n",
    "    train_data = ListDataset(list(iter(dataset.train)), freq=dataset.metadata.freq)\n",
    "    validation_data = []\n",
    "    for i in range(num_validation_windows):\n",
    "        for ts in train_data.list_data:\n",
    "            # only add time series long enough that we can remove one horizon and still have context_length +\n",
    "            # prediction_length values left\n",
    "            if len(ts[\"target\"]) <= context_length + prediction_length * (i + 2):\n",
    "                continue\n",
    "\n",
    "            val_ts = deepcopy(ts)\n",
    "            val_ts[\"target\"] = val_ts[\"target\"][:-prediction_length * i if i > 0 else None]\n",
    "            val_ts[\"target\"] = val_ts[\"target\"][-(context_length + prediction_length):]\n",
    "            validation_data.append(val_ts)\n",
    "\n",
    "            # slice off the validation data from the training data\n",
    "            if i == num_validation_windows - 1:\n",
    "                ts[\"target\"] = ts[\"target\"][:-prediction_length * (i + 1)]\n",
    "    \n",
    "    return train_data, ListDataset(validation_data, freq=dataset.metadata.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2fddc57-dd93-4d88-92a5-f4912c102063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dataset(original_data, generated_data):\n",
    "    for i, ts in enumerate(generated_data):\n",
    "        entry = {\"start\": ts.index[0], \"target\": ts.values, \"feat_static_cat\": np.array([i]), \"item_id\": i}\n",
    "        original_data.list_data.append(entry)\n",
    "    \n",
    "    return original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a00e654e-4f55-4121-82ed-f9f59e49e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_trend_lin(data, sp, h):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        if sp > 1:\n",
    "            decomp = STL(ts, period=sp).fit()\n",
    "        else:\n",
    "            decomp = extract_trend(ts)\n",
    "        \n",
    "        new_trend = manipulate_trend_component(decomp.trend, f=1, g=1, h=h, m=0)\n",
    "        new_ts = new_trend + decomp.seasonal + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "def change_trend_str(data, sp, f):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        if sp > 1:\n",
    "            decomp = STL(ts, period=sp).fit()\n",
    "        else:\n",
    "            decomp = extract_trend(ts)\n",
    "        \n",
    "        new_trend = manipulate_trend_component(decomp.trend, f=f, g=1, h=1, m=0)\n",
    "        new_ts = new_trend + decomp.seasonal + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "def change_trend_slope(data, sp, m):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        if sp > 1:\n",
    "            decomp = STL(ts, period=sp).fit()\n",
    "        else:\n",
    "            decomp = extract_trend(ts)\n",
    "        \n",
    "        new_trend = manipulate_trend_component(decomp.trend, f=1, g=1, h=1, m=m)\n",
    "        new_ts = new_trend + decomp.seasonal + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "def change_seas_str(data, sp, k):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        if sp > 1:\n",
    "            decomp = STL(ts, period=sp).fit()\n",
    "        else:\n",
    "            decomp = extract_trend(ts)\n",
    "        \n",
    "        new_season= manipulate_seasonal_determination(decomp.seasonal, k=k)\n",
    "        new_ts = decomp.trend + new_season + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "007e0fb6-11d9-4086-a728-74b6c485d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(config[\"dataset\"])\n",
    "train_data, validation_data = get_train_and_val_data(dataset, 1, config[\"trainer_args\"][\"context_length\"],\n",
    "                                                     config[\"trainer_args\"][\"prediction_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9ce0e4-3cd6-468a-b0cb-dec083a83bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23000it [01:12, 317.42it/s]\n"
     ]
    }
   ],
   "source": [
    "generated_train = change_trend_slope(train_data, config[\"sp\"], m=-10)\n",
    "expanded_train = add_to_dataset(train_data, generated_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9328f343-e85b-400c-8392-87125d1bdf0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-24bbc3916ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_trend_slope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexpanded_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"
     ]
    }
   ],
   "source": [
    "generated_val = change_trend_slope(validation_data, config[\"sp\"], h-1)\n",
    "expanded_val = add_to_dataset(validation_data, generated_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffc6c3-ca73-4e53-bafa-1cc1404388d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(expanded_train, config[\"trainer_args\"][\"context_length\"], config[\"trainer_args\"][\"prediction_length\"],\n",
    "                                           config[\"trainer_args\"][\"batch_size\"], config[\"trainer_args\"][\"num_batches_per_epoch\"] * 2)\n",
    "\n",
    "validation_dataloader = create_validation_dataloader(expanded_val, config[\"trainer_args\"][\"context_length\"], config[\"trainer_args\"][\"prediction_length\"],\n",
    "                                                     config[\"trainer_args\"][\"batch_size\"], config[\"trainer_args\"][\"num_batches_per_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec2341-c1c9-43f1-a766-302d723cb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sample_and_plot_from_dataloader(train_dataloader, config[\"sp\"], scaler, pca, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fda3a4-5884-495e-9e47-4cac94c904e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sample_and_plot_from_dataloader(validation_dataloader, config[\"sp\"], scaler, pca, None, None, num_samples=len(expanded_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71087e41-4253-45bb-833e-71ba1922cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
