{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a969d5-62ac-4b69-810d-cd723e9f3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590ce29c-8176-47a7-863e-b6a94dd65f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/whatif/lib/python3.8/site-packages/gluonts/json.py:45: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.loader import TrainDataLoader, ValidationDataLoader\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.time_feature import (\n",
    "    HourOfDay,\n",
    "    DayOfWeek,\n",
    "    DayOfMonth,\n",
    "    DayOfYear,\n",
    "    MonthOfYear\n",
    ")\n",
    "from gluonts.torch.batchify import batchify\n",
    "from gluonts.transform import (\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    ValidationSplitSampler\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.app.utils import get_prediction_dataloader\n",
    "from src.models.utils import get_model\n",
    "from src.utils.data_loading import load_features, load_score, load_test_data, load_train_data\n",
    "from src.utils.evaluation import score_batch\n",
    "from src.utils.features import trend_determination, trend_slope, trend_linearity, seasonal_determination, decomps_and_features\n",
    "from src.utils.transformations import manipulate_trend_component, manipulate_seasonal_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf78e58-abf1-4f0b-a5a1-4a9d3368f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nbeats_g\"\n",
    "dataset = \"m4_weekly\"\n",
    "\n",
    "datadir = f\"data/{dataset}\"\n",
    "experiment_dir = f\"experiments/{dataset}/{model_name}\"\n",
    "generated_datadir = os.path.join(f\"/datadrive2/whatif/{dataset}\", \"generated\", \"test\")\n",
    "metric = \"mase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d4ab5f-d442-4e7d-95eb-f67bb76b1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_dir, \"config.yaml\"), \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b67b45-6d97-41e3-a606-1628b5d8cb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = load_score(experiment_dir, metric)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e43b8b-52e4-4e88-b5d2-c8d8d91cb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(dataset, config):\n",
    "    test_data = load_test_data(dataset, config[\"context_length\"] + config[\"prediction_length\"])\n",
    "    \n",
    "    trend_str_inc_ts = []\n",
    "    trend_str_dec_ts = []\n",
    "    trend_lin_inc_ts = []\n",
    "    trend_lin_dec_ts = []\n",
    "    trend_slope_inc_ts = []\n",
    "    trend_slope_dec_ts = []\n",
    "    seas_str_inc_ts = []\n",
    "    seas_str_dec_ts = []\n",
    "\n",
    "    trend_str_inc_feat = []\n",
    "    trend_str_dec_feat = []\n",
    "    trend_lin_inc_feat = []\n",
    "    trend_lin_dec_feat = []\n",
    "    trend_slope_inc_feat = []\n",
    "    trend_slope_dec_feat = []\n",
    "    seas_str_inc_feat = []\n",
    "    seas_str_dec_feat = []\n",
    "    for ts in tqdm(test_data):\n",
    "        decomp = decomps_and_features([ts], config[\"sp\"])[0][0]\n",
    "        \n",
    "        inc_str = manipulate_trend_component(decomp.trend, f=2, g=1, h=1, m=0)\n",
    "        dec_str = manipulate_trend_component(decomp.trend, f=0.01, g=1, h=1, m=0)\n",
    "        \n",
    "        inc_lin = manipulate_trend_component(decomp.trend, f=1, g=1, h=2, m=0)\n",
    "        dec_lin = manipulate_trend_component(decomp.trend, f=1, g=1, h=0.01, m=0)\n",
    "        \n",
    "        inc_slope = manipulate_trend_component(decomp.trend, f=1, g=1, h=1, m=-1)\n",
    "        dec_slope = manipulate_trend_component(decomp.trend, f=1, g=1, h=1, m=1)\n",
    "        \n",
    "        generated_ts = [inc_str, dec_str, inc_lin, dec_lin, inc_slope, dec_slope]\n",
    "        if config[\"sp\"] > 1:\n",
    "            inc_seas = manipulate_seasonal_determination(decomp.seasonal, k=2)\n",
    "            dec_seas = manipulate_seasonal_determination(decomp.seasonal, k=0.01)\n",
    "            generated_ts.extend([inc_seas, dec_seas])\n",
    "            \n",
    "        _, features = decomps_and_features(generated_ts, config[\"sp\"])\n",
    "        \n",
    "        trend_str_inc_ts.append(inc_str)\n",
    "        trend_str_dec_ts.append(dec_str)\n",
    "        trend_lin_inc_ts.append(inc_lin)\n",
    "        trend_lin_dec_ts.append(dec_lin)\n",
    "        trend_slope_inc_ts.append(inc_slope)\n",
    "        trend_slope_dec_ts.append(dec_slope)\n",
    "        \n",
    "        trend_str_inc_feat.append(features[0])\n",
    "        trend_str_dec_feat.append(features[1])\n",
    "        trend_lin_inc_feat.append(features[2])\n",
    "        trend_lin_dec_feat.append(features[3])\n",
    "        trend_slope_inc_feat.append(features[4])\n",
    "        trend_slope_dec_feat.append(features[5])\n",
    "        \n",
    "        if config[\"sp\"] > 1:\n",
    "            seas_str_inc_ts.append(inc_seas)\n",
    "            seas_str_dec_ts.append(dec_seas)\n",
    "            seas_str_inc_feat.append(features[6])\n",
    "            seas_str_dec_feat.append(features[7])\n",
    "    \n",
    "    ts_dict = {\"str_inc\": trend_str_inc_ts, \"str_dec\": trend_str_dec_ts,\n",
    "               \"lin_inc\": trend_lin_inc_ts, \"lin_dec\": trend_lin_dec_ts,\n",
    "               \"slope_inc\": trend_slope_inc_ts, \"slope_dec\": trend_slope_dec_ts,\n",
    "               \"seas_inc\": seas_str_inc_ts, \"seas_dec\": seas_str_dec_ts}\n",
    "    \n",
    "    feat_dict = {\"str_inc\": trend_str_inc_feat, \"str_dec\": trend_str_dec_feat,\n",
    "                \"lin_inc\": trend_lin_inc_feat, \"lin_dec\": trend_lin_dec_feat,\n",
    "                \"slope_inc\": trend_slope_inc_feat, \"slope_dec\": trend_slope_dec_feat,\n",
    "                \"seas_inc\": seas_str_inc_feat, \"seas_dec\": seas_str_dec_feat}\n",
    "    \n",
    "    return ts_dict, feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01536b7f-ac64-4300-bd25-8d2ece857196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 113/359 [00:00<00:00, 1124.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:00<00:00, 1108.81it/s]\n",
      " 49%|████▉     | 176/359 [00:05<00:05, 35.13it/s]"
     ]
    }
   ],
   "source": [
    "generated_data, generated_features = generate_data(dataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf645d6a-1f25-4b45-9f5e-89b1ee9f22c0",
   "metadata": {},
   "source": [
    "### Can we use the already generated data to find out of distribution samples and use those as testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98aa35-fbef-4397-9d61-f9017414f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = load_features(datadir, train=True)\n",
    "test_features = load_features(datadir, train=False)\n",
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d4000-0383-4684-ad7b-c4b228411f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_features = np.array(generated_features[\"slope_inc\"])\n",
    "gen_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bd136-9753-4149-9434-2912c19fbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "norm_train_features = scaler.fit_transform(train_features)\n",
    "norm_test_features = scaler.transform(test_features)\n",
    "norm_gen_features = scaler.transform(gen_features)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "train_pca_data = pca.fit_transform(norm_train_features)\n",
    "test_pca_data = pca.transform(norm_test_features)\n",
    "gen_pca_data = pca.transform(norm_gen_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b4d7c-c242-4a87-9ced-123776427f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.scatter(gen_pca_data[:, 0], gen_pca_data[:, 1], label=\"generated data\", s=5, alpha=0.5)\n",
    "plt.scatter(train_pca_data[:, 0], train_pca_data[:, 1], label=\"train data\", s=5, alpha=0.5)\n",
    "plt.scatter(test_pca_data[:, 0], test_pca_data[:, 1], label=\"test data\", s=5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlim([-10, 10])\n",
    "plt.ylim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be8d4d-6d6c-4687-8ea0-8a48c09dcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_plot_from_dataloader(dataloader, sp, scaler, pca, train_pca_data, test_pca_data, num_samples=20000, lim=True):\n",
    "    sampled_series = []\n",
    "    for batch in dataloader:\n",
    "        data = torch.cat([batch[\"past_target\"], batch[\"future_target\"]], dim=-1)\n",
    "        for ts in data:\n",
    "            sampled_series.append(pd.Series(ts))\n",
    "\n",
    "        if len(sampled_series) > num_samples:\n",
    "            break\n",
    "    \n",
    "    _, sampled_features = decomps_and_features(sampled_series, sp)\n",
    "    sampled_scaled_features = scaler.transform(sampled_features)\n",
    "    sampled_pca_data = pca.transform(sampled_scaled_features)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    if train_pca_data is not None:\n",
    "        plt.scatter(train_pca_data[:, 0], train_pca_data[:, 1], label=\"train data\", s=5, alpha=.1)\n",
    "    if test_pca_data is not None:\n",
    "        plt.scatter(test_pca_data[:, 0], test_pca_data[:, 1], label=\"test data\", s=5, alpha=.1)\n",
    "    \n",
    "    plt.scatter(sampled_pca_data[:, 0], sampled_pca_data[:, 1], label=\"transformed train data\", s=5, alpha=.1, color=\"r\")\n",
    "    plt.legend()\n",
    "    \n",
    "    if lim:\n",
    "        plt.xlim([-10, 10])\n",
    "        plt.ylim([-10, 10])\n",
    "    \n",
    "    return sampled_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb277a-c2a7-4539-89ed-964611552bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataloader(dataset, context_length, prediction_length, batch_size, num_batches_per_epoch):\n",
    "    transformation = Chain([\n",
    "        AddObservedValuesIndicator(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.OBSERVED_VALUES,\n",
    "        ),\n",
    "        AddTimeFeatures(\n",
    "            start_field=FieldName.START,\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            pred_length=prediction_length,\n",
    "            time_features=[HourOfDay(), DayOfWeek(), DayOfMonth(), DayOfYear(), MonthOfYear()]\n",
    "        ),\n",
    "        InstanceSplitter(\n",
    "            target_field=FieldName.TARGET,\n",
    "            is_pad_field=FieldName.IS_PAD,\n",
    "            start_field=FieldName.START,\n",
    "            forecast_start_field=FieldName.FORECAST_START,\n",
    "            instance_sampler=ExpectedNumInstanceSampler(num_instances=1, min_past=context_length,\n",
    "                                                        min_future=prediction_length),\n",
    "            past_length=context_length,\n",
    "            future_length=prediction_length,\n",
    "            time_series_fields=[FieldName.FEAT_TIME, FieldName.OBSERVED_VALUES]\n",
    "        )\n",
    "    ])\n",
    "    dataloader = TrainDataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        stack_fn=batchify,\n",
    "        transform=transformation,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "        num_workers=1\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63b0d8-7304-47c5-b1b5-16333cbea15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_dataloader(dataset, context_length, prediction_length, batch_size, num_batches_per_epoch):\n",
    "    transformation = Chain([\n",
    "        AddObservedValuesIndicator(\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.OBSERVED_VALUES,\n",
    "        ),\n",
    "        AddTimeFeatures(\n",
    "            start_field=FieldName.START,\n",
    "            target_field=FieldName.TARGET,\n",
    "            output_field=FieldName.FEAT_TIME,\n",
    "            pred_length=prediction_length,\n",
    "            time_features=[HourOfDay(), DayOfWeek(), DayOfMonth(), DayOfYear(), MonthOfYear()]\n",
    "        ),\n",
    "        InstanceSplitter(\n",
    "            target_field=FieldName.TARGET,\n",
    "            is_pad_field=FieldName.IS_PAD,\n",
    "            start_field=FieldName.START,\n",
    "            forecast_start_field=FieldName.FORECAST_START,\n",
    "            instance_sampler=ValidationSplitSampler(min_future=prediction_length),\n",
    "            past_length=context_length,\n",
    "            future_length=prediction_length,\n",
    "            time_series_fields=[FieldName.FEAT_TIME, FieldName.OBSERVED_VALUES]\n",
    "        )\n",
    "    ])\n",
    "    dataloader = ValidationDataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        stack_fn=batchify,\n",
    "        transform=transformation,\n",
    "        num_workers=1\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90696a-7328-4468-8ca9-61930282a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_val_data(dataset, num_validation_windows, context_length, prediction_length):\n",
    "    train_data = ListDataset(list(iter(dataset.train)), freq=dataset.metadata.freq)\n",
    "    validation_data = []\n",
    "    for i in range(num_validation_windows):\n",
    "        for ts in train_data.list_data:\n",
    "            # only add time series long enough that we can remove one horizon and still have context_length +\n",
    "            # prediction_length values left\n",
    "            if len(ts[\"target\"]) <= context_length + prediction_length * (i + 2):\n",
    "                continue\n",
    "\n",
    "            val_ts = deepcopy(ts)\n",
    "            val_ts[\"target\"] = val_ts[\"target\"][:-prediction_length * i if i > 0 else None]\n",
    "            val_ts[\"target\"] = val_ts[\"target\"][-(context_length + prediction_length):]\n",
    "            validation_data.append(val_ts)\n",
    "\n",
    "            # slice off the validation data from the training data\n",
    "            if i == num_validation_windows - 1:\n",
    "                ts[\"target\"] = ts[\"target\"][:-prediction_length * (i + 1)]\n",
    "    \n",
    "    return train_data, ListDataset(validation_data, freq=dataset.metadata.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fddc57-dd93-4d88-92a5-f4912c102063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dataset(original_data, generated_data):\n",
    "    for i, ts in enumerate(generated_data):\n",
    "        entry = {\"start\": ts.index[0], \"target\": ts.values, \"feat_static_cat\": np.array([i]), \"item_id\": i}\n",
    "        original_data.list_data.append(entry)\n",
    "    \n",
    "    return original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e654e-4f55-4121-82ed-f9f59e49e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_trend_lin(data, sp, h):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        decomp = decomps_and_features([ts], config[\"sp\"])[0][0]\n",
    "        \n",
    "        new_trend = manipulate_trend_component(decomp.trend, f=1, g=1, h=h, m=0)\n",
    "        new_ts = new_trend + decomp.seasonal + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "def change_trend_str(data, sp, f):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        decomp = decomps_and_features([ts], config[\"sp\"])[0][0]\n",
    "        \n",
    "        new_trend = manipulate_trend_component(decomp.trend, f=f, g=1, h=1, m=0)\n",
    "        new_ts = new_trend + decomp.seasonal + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "def change_trend_slope(data, sp, m):\n",
    "    generated = []\n",
    "    for i, ts in tqdm(enumerate(data)):\n",
    "        index = pd.date_range(start=ts[\"start\"], freq=ts[\"start\"].freq, periods=len(ts[\"target\"]))\n",
    "        ts = pd.Series(data=ts[\"target\"], index=index)\n",
    "        decomp = decomps_and_features([ts], config[\"sp\"])[0][0]\n",
    "        \n",
    "        new_trend = manipulate_trend_component(decomp.trend, f=1, g=1, h=1, m=m)\n",
    "        new_ts = new_trend + decomp.seasonal + decomp.resid\n",
    "        generated.append(new_ts)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e0fb6-11d9-4086-a728-74b6c485d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(config[\"dataset\"])\n",
    "train_data, validation_data = get_train_and_val_data(dataset, 1, config[\"trainer_args\"][\"context_length\"],\n",
    "                                                     config[\"trainer_args\"][\"prediction_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ce0e4-3cd6-468a-b0cb-dec083a83bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train = change_trend_slope(train_data, config[\"sp\"], m=10)\n",
    "expanded_train = add_to_dataset(train_data, generated_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328f343-e85b-400c-8392-87125d1bdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_val = change_trend_slope(validation_data, config[\"sp\"], m=1)\n",
    "expanded_val = add_to_dataset(validation_data, generated_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffc6c3-ca73-4e53-bafa-1cc1404388d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(expanded_train, config[\"trainer_args\"][\"context_length\"], config[\"trainer_args\"][\"prediction_length\"],\n",
    "                                           config[\"trainer_args\"][\"batch_size\"], config[\"trainer_args\"][\"num_batches_per_epoch\"])\n",
    "\n",
    "validation_dataloader = create_validation_dataloader(expanded_val, config[\"trainer_args\"][\"context_length\"], config[\"trainer_args\"][\"prediction_length\"],\n",
    "                                                     config[\"trainer_args\"][\"batch_size\"], config[\"trainer_args\"][\"num_batches_per_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec2341-c1c9-43f1-a766-302d723cb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sample_and_plot_from_dataloader(train_dataloader, config[\"sp\"], scaler, pca, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fda3a4-5884-495e-9e47-4cac94c904e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sample_and_plot_from_dataloader(validation_dataloader, config[\"sp\"], scaler, pca, None, None, num_samples=len(expanded_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71087e41-4253-45bb-833e-71ba1922cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
